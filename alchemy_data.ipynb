{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from alchemy_api import AlchemyApi\n",
    "import db_connection as db_conn\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import backoff\n",
    "from requests.exceptions import RequestException\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import threading\n",
    "import pickle\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, when, lit, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, NullType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alchemy = AlchemyApi()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transaction History Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_key_rotation(api_keys):\n",
    "    index = random.randint(0, len(api_keys) - 1)\n",
    "    api_key = api_keys[index]\n",
    "    return api_key\n",
    "\n",
    "def get_block_chunks(start_block, end_block, step=3):\n",
    "    ranges = []\n",
    "    for i in range(start_block, end_block - step + 2, step):\n",
    "        ranges.append((i, i + step - 1))\n",
    "    if ranges[-1][1] < end_block:\n",
    "        ranges.append((ranges[-1][1] + 1, end_block))\n",
    "    return ranges\n",
    "\n",
    "@backoff.on_exception(backoff.expo, \n",
    "                      (RequestException, KeyError), \n",
    "                      max_tries=4)\n",
    "def fetch_transactions(block_range):\n",
    "    transactions = []\n",
    "    failed_ranges = []\n",
    "    page_key = None\n",
    "    while True:\n",
    "        try:\n",
    "            start_block_hex = alchemy.convert_block_to_hex(block_range[0])\n",
    "            end__block_hex = alchemy.convert_block_to_hex(block_range[1])\n",
    "            api_key = api_key_rotation(alchemy.get_api_keys())\n",
    "            alchemy_url = alchemy.get_api_url() + api_key\n",
    "            transfers_body = alchemy.create_asset_transfers()\n",
    "            transfers_body = alchemy.set_tx_block_range(transfers_body, block_from=start_block_hex, block_to=end__block_hex)\n",
    "            if page_key:\n",
    "                transfers_body = alchemy.set_tx_pagination(transfers_body, page_key=page_key)\n",
    "            response_tx = requests.post(alchemy_url, json=transfers_body)\n",
    "            response_tx_json = response_tx.json()\n",
    "            page_key = response_tx_json[\"result\"][\"pageKey\"]\n",
    "            transactions = [{\"hash\": tx[\"hash\"], \"blockNumber\": tx[\"blockNum\"], \"from\": tx[\"from\"], \"to\": tx[\"to\"], \"value\": tx[\"value\"], \"erc721TokenId\": tx[\"erc721TokenId\"], \"erc1155Metadata\": tx[\"erc1155Metadata\"], \n",
    "                            \"tokenId\": tx[\"tokenId\"], \"asset\": tx[\"asset\"], \"category\": tx[\"category\"], \"timestamp\": tx['metadata']['blockTimestamp']} for tx in response_tx_json[\"result\"][\"transfers\"]]\n",
    "            if \"pageKey\" not in response_tx_json[\"result\"]:\n",
    "                break\n",
    "            page_key = response_tx_json[\"result\"][\"pageKey\"]\n",
    "        except (RequestException, KeyError) as e:\n",
    "            print(f\"Request failed with exception: {e}. Retrying...\")\n",
    "            print(f\"API key used: {api_key}\")\n",
    "            failed_ranges.append(block_range)\n",
    "            raise\n",
    "\n",
    "    return transactions, failed_ranges\n",
    "\n",
    "@backoff.on_exception(backoff.expo, \n",
    "                      (RequestException, KeyError), \n",
    "                      max_tries=4)\n",
    "def fetch_io_transactions_by_address(address, transaction_direction):\n",
    "    from_transactions = []\n",
    "    failed_from_addresses = []\n",
    "    page_key = None\n",
    "    page_counter = 1\n",
    "    while True:\n",
    "        try:\n",
    "            api_key = api_key_rotation(alchemy.get_api_keys())\n",
    "            alchemy_url = alchemy.get_api_url() + api_key\n",
    "            transfers_body = alchemy.create_asset_transfers()\n",
    "            if transaction_direction == \"from\":\n",
    "                transfers_body = alchemy.set_tx_address(transfers_body, address_from=address, address_to=None)\n",
    "            else:\n",
    "                transfers_body = alchemy.set_tx_address(transfers_body, address_from=None, address_to=address)\n",
    "            if page_key:\n",
    "                transfers_body = alchemy.set_tx_pagination(transfers_body, page_key=page_key)\n",
    "            response_tx = requests.post(alchemy_url, json=transfers_body)\n",
    "            response_tx_json = response_tx.json()\n",
    "            transactions = [{\"hash\": tx[\"hash\"], \"blockNumber\": tx[\"blockNum\"], \"from\": tx[\"from\"], \"to\": tx[\"to\"], \n",
    "                            \"value\": tx[\"value\"], \"erc721TokenId\": tx[\"erc721TokenId\"], \n",
    "                            \"erc1155Metadata\": tx[\"erc1155Metadata\"], \"tokenId\": tx[\"tokenId\"], \n",
    "                            \"asset\": tx[\"asset\"], \"category\": tx[\"category\"], \n",
    "                            \"timestamp\": tx['metadata']['blockTimestamp']} for tx in response_tx_json[\"result\"][\"transfers\"]]\n",
    "            from_transactions.extend(transactions)\n",
    "            \n",
    "            if \"pageKey\" not in response_tx_json[\"result\"]:\n",
    "                break\n",
    "            page_counter += 1\n",
    "            page_key = response_tx_json[\"result\"][\"pageKey\"]\n",
    "        except (RequestException, KeyError) as e:\n",
    "            print('From {} page {}: {}'.format(address, page_counter, response_tx_json))\n",
    "            print(f\"Request failed with exception: {e}. Retrying...\")\n",
    "            print(f\"API key used: {api_key}\")\n",
    "            failed_from_addresses.append(address)\n",
    "            raise\n",
    "\n",
    "    return from_transactions, failed_from_addresses\n",
    "\n",
    "def fetch_transactions_by_address(address):\n",
    "    to_transactions, failed_to_addresses = fetch_io_transactions_by_address(address, 'to')\n",
    "    from_transactions, failed_from_addresses = fetch_io_transactions_by_address(address, 'from')\n",
    "    return to_transactions, from_transactions, failed_to_addresses, failed_from_addresses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transaction History by Block Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_block = 14814062\n",
    "end_block = 14881676\n",
    "block_ranges = get_block_chunks(start_block, end_block, 3)\n",
    "\n",
    "all_transactions = []\n",
    "all_failed_ranges = []\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "    futures = [executor.submit(fetch_transactions, block_range) for block_range in block_ranges]\n",
    "    for future in as_completed(futures):\n",
    "        transactions, failed_ranges = future.result()\n",
    "        with lock:\n",
    "            all_transactions.extend(transactions)\n",
    "            all_failed_ranges.extend(failed_ranges)\n",
    "\n",
    "print(len(all_transactions))\n",
    "transaction_blocks = set(tx['blockNumber'] for tx in all_transactions)\n",
    "# Filter out the failed ranges that have block numbers in all transactions\n",
    "all_failed_ranges = [block_range for block_range in all_failed_ranges \n",
    "                     if alchemy.convert_block_to_hex(block_range[0]) \n",
    "                     not in transaction_blocks and alchemy.convert_block_to_hex(block_range[1]) not in transaction_blocks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(all_failed_ranges)\n",
    "with open('data/pickle_files/all_transactions_may_3.pickle', 'wb') as f:\n",
    "    pickle.dump(all_transactions, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transaction History by account number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_addresses = pd.read_pickle('data/pickle_files/scam_users_and_contract_creators_alt.pkl')\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "random.shuffle(all_addresses)\n",
    "addresses_chunks = np.array_split(all_addresses, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_transactions_by_address = []\n",
    "failed_addresses = []\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "    futures = [executor.submit(fetch_transactions_by_address, address) for address in addresses_chunks[2]]\n",
    "    for future in as_completed(futures):\n",
    "        to_transactions, from_transactions, failed_to_addresses, failed_from_addresses = future.result()\n",
    "        with lock:\n",
    "            all_transactions_by_address.extend(from_transactions)\n",
    "            failed_addresses.extend(failed_from_addresses)\n",
    "            all_transactions_by_address.extend(to_transactions)\n",
    "            failed_addresses.extend(failed_to_addresses)\n",
    "\n",
    "from_addresses = set(tx['from'] for tx in all_transactions_by_address)\n",
    "to_addresses = set(tx['to'] for tx in all_transactions_by_address)\n",
    "transaction_addresses = from_addresses.union(to_addresses)\n",
    "# Filter out the failed ranges that have block numbers in all transactions\n",
    "all_failed_addresses = [address for address in failed_addresses if address not in transaction_addresses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8730326"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_transactions_by_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(all_failed_addresses)\n",
    "with open('data/pickle_files/all_transactions_scam_3_alt.pickle', 'wb') as f:\n",
    "    pickle.dump(all_transactions_by_address, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20074182\n"
     ]
    }
   ],
   "source": [
    "# df_part1 = pd.DataFrame(pd.read_pickle('data/pickle_files/all_transactions_may_1.pickle'))\n",
    "# print(df_part1.shape[0])\n",
    "# df_part1.to_parquet('data/parquet_files/may1_tx.parquet')\n",
    "# df_part1 = None\n",
    "# df_part2 = pd.DataFrame(pd.read_pickle('data/pickle_files/all_transactions_may_2.pickle'))\n",
    "# print(df_part2.shape[0])\n",
    "# df_part2.to_parquet('data/parquet_files/may2_tx.parquet')\n",
    "# df_part2 = None\n",
    "# df_part3 = pd.DataFrame(pd.read_pickle('data/pickle_files/all_transactions_may_3.pickle'))\n",
    "# print(df_part3.shape[0])\n",
    "# df_part3.to_parquet('data/parquet_files/may3_tx.parquet')\n",
    "# df_part3 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "792932\n",
      "420029\n",
      "8730326\n"
     ]
    }
   ],
   "source": [
    "df_part1 = pd.DataFrame(pd.read_pickle('data/pickle_files/all_transactions_scam_1_alt.pickle'))\n",
    "print(df_part1.shape[0])\n",
    "df_part1.to_parquet('data/parquet_files/scam_tx_1_alt.parquet')\n",
    "df_part2 = pd.DataFrame(pd.read_pickle('data/pickle_files/all_transactions_scam_2_alt.pickle'))\n",
    "print(df_part2.shape[0])\n",
    "df_part2.to_parquet('data/parquet_files/scam_tx_2_alt.parquet')\n",
    "df_part3 = pd.DataFrame(pd.read_pickle('data/pickle_files/all_transactions_scam_3_alt.pickle'))\n",
    "print(df_part3.shape[0])\n",
    "df_part3.to_parquet('data/parquet_files/scam_tx_3_alt.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/04 12:44:36 WARN Utils: Your hostname, NatRng-MBP.local resolves to a loopback address: 127.0.0.1; using 10.200.168.84 instead (on interface en0)\n",
      "23/08/04 12:44:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/04 12:44:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"process_df_tx\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memory\", \"32g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# schema of txdata\n",
    "schema = StructType([\n",
    "    StructField(\"hash\", StringType()),\n",
    "    StructField(\"blockNumber\", StringType()),\n",
    "    StructField(\"from\", StringType()),\n",
    "    StructField(\"to\", StringType()),\n",
    "    StructField(\"value\", FloatType()),\n",
    "    StructField(\"erc721TokenId\", StringType()),\n",
    "    StructField(\"erc1155Metadata\", StringType()),\n",
    "    StructField(\"tokenId\", StringType()),\n",
    "    StructField(\"asset\", StringType()),\n",
    "    StructField(\"category\", StringType()),\n",
    "    StructField(\"timestamp\", StringType())\n",
    "])\n",
    "\n",
    "#merge parts\n",
    "df_tx_1 = spark.read.parquet('data/parquet_files/scam_tx_1_alt.parquet')\n",
    "df_tx_2 = spark.read.parquet('data/parquet_files/scam_tx_2_alt.parquet')\n",
    "df_tx_3 = spark.read.parquet('data/parquet_files/scam_tx_3_alt.parquet')\n",
    "df_tx = df_tx_1.union(df_tx_2).union(df_tx_3)\n",
    "df_tx = df_tx.repartition(100)\n",
    "num_rows = df_tx.filter(df_tx[\"erc1155Metadata\"].isNotNull()).count()\n",
    "\n",
    "if num_rows > 0:\n",
    "    df_tx = df_tx.withColumn('erc1155_token_id', when(col('erc1155Metadata').isNotNull() & (col('erc1155Metadata').getItem(0).isNotNull()), col('erc1155Metadata').getItem(0).getItem('tokenId')).otherwise(lit(None)))\n",
    "    df_tx = df_tx.withColumn('erc1155_value', when(col('erc1155Metadata').isNotNull() & (col('erc1155Metadata').getItem(0).isNotNull()), col('erc1155Metadata').getItem(0).getItem('value')).otherwise(lit(None)))\n",
    "    df_tx = df_tx.drop('erc1155Metadata')\n",
    "else:\n",
    "    df_tx = df_tx.withColumn('erc1155_token_id', lit(None))\n",
    "    df_tx = df_tx.withColumn('erc1155_value', lit(None))\n",
    "    df_tx = df_tx.drop('erc1155Metadata')\n",
    "\n",
    "#process data types\n",
    "df_tx = df_tx.withColumn(\"value\", col(\"value\").cast(\"float\"))\n",
    "df_tx = df_tx.withColumn(\"erc1155_value\", col(\"erc1155_value\").cast(\"float\"))\n",
    "df_tx = df_tx.withColumn('timestamp', to_timestamp(col('timestamp'), \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"))\n",
    "df_tx = df_tx.select(['hash', 'blockNumber', 'from', 'to', 'value', 'erc721TokenId', 'erc1155_token_id', 'erc1155_value', 'tokenId', 'asset', 'category', 'timestamp'])\n",
    "df_tx = df_tx.withColumnRenamed('hash', 'tx_hash').withColumnRenamed('value', 'asset_value') \\\n",
    "        .withColumnRenamed('from', 'from_address').withColumnRenamed('to', 'to_address') \\\n",
    "        .withColumnRenamed('erc721TokenId', 'erc721_token_id').withColumnRenamed('tokenId', 'token_id') \\\n",
    "        .withColumnRenamed('blockNumber', 'block_number')\n",
    "\n",
    "df_tx = df_tx.dropDuplicates()\n",
    "\n",
    "df_blocks = df_tx.select('block_number').distinct()\n",
    "\n",
    "df_addresses = df_tx.select('from_address').union(df_tx.select('to_address')).distinct().dropna()\n",
    "df_addresses = df_addresses.withColumnRenamed('from_address', 'address')\n",
    "\n",
    "df_categories = df_tx.select('category').distinct()\n",
    "df_categories = df_categories.withColumnRenamed('category', 'category_name')\n",
    "\n",
    "df_contracts = df_tx.select('tx_hash', 'block_number', 'from_address', 'to_address').distinct()\n",
    "df_contracts = df_contracts.filter(df_contracts['to_address'].isNull())\n",
    "df_contracts = df_contracts.select('tx_hash', 'block_number', 'from_address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_config = db_conn.config_gsql\n",
    "db_config = db_conn.config_scam_alt_sql\n",
    "url = f\"jdbc:mariadb://{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "user = db_config['user']\n",
    "password = db_config['password']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocks table exported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addresses table exported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:=========================================>            (77 + 10) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TxCategories table exported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cnx, cursor = db_conn.connect_db(db_config)\n",
    "db_conn.create_primary_tables(cursor)\n",
    "cursor.close()\n",
    "cnx.close()\n",
    "\n",
    "db_conn.export_df_to_sql(df_blocks, 'Blocks', db_config)\n",
    "print('Blocks table exported')\n",
    "db_conn.export_df_to_sql(df_addresses, 'Addresses', db_config)\n",
    "print('Addresses table exported')\n",
    "db_conn.export_df_to_sql(df_categories, 'TxCategories', db_config)\n",
    "print('TxCategories table exported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_df = spark.read.format('jdbc').options(url=url, dbtable='Blocks', user=user, password=password).load()\n",
    "address_df = spark.read.format('jdbc').options(url=url, dbtable='Addresses', user=user, password=password).load()\n",
    "category_df = spark.read.format('jdbc').options(url=url, dbtable='TxCategories', user=user, password=password).load()\n",
    "\n",
    "df_tx = df_tx.join(block_df, df_tx[\"block_number\"] == block_df[\"block_number\"], 'left') \\\n",
    "    .select(df_tx[\"*\"], block_df['block_id'])\n",
    "\n",
    "df_tx = df_tx.join(address_df.alias('from_address_df'), df_tx.from_address == col(\"from_address_df.address\"), 'left') \\\n",
    "    .select(df_tx[\"*\"], col('from_address_df.address_id').alias('from_id'))\n",
    "\n",
    "df_tx = df_tx.join(address_df.alias('to_address_df'), df_tx.to_address == col(\"to_address_df.address\"), 'left') \\\n",
    "    .select(df_tx[\"*\"], col('to_address_df.address_id').alias('to_id'))\n",
    "\n",
    "df_tx = df_tx.join(category_df, df_tx.category == category_df.category_name, 'left') \\\n",
    "    .select(df_tx[\"*\"], category_df['category_id'])\n",
    "\n",
    "df_tx = df_tx.drop('block_number', 'from_address', 'to_address', 'category', 'address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tx = df_tx.select(['tx_hash', 'block_id', 'from_id', 'to_id', 'asset_value', 'erc721_token_id', 'erc1155_token_id', 'erc1155_value', 'token_id', 'asset', 'category_id', 'timestamp'])\n",
    "# df_tx.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tx = df_tx.withColumn('erc1155_token_id', col('erc1155_token_id').cast('string'))\n",
    "df_tx = df_tx.withColumn('erc1155_value', col('erc1155_value').cast('string'))\n",
    "\n",
    "df_tx = df_tx.fillna({'erc1155_token_id': '', 'erc1155_value': ''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cnx, cursor = db_conn.connect_db(db_config)\n",
    "db_conn.create_transactions_table(cursor)\n",
    "cursor.close()\n",
    "cnx.close()\n",
    "\n",
    "db_conn.export_df_to_sql(df_tx, 'Transactions', db_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contracts = df_contracts.join(address_df, df_contracts.from_address == address_df.address, 'left') \\\n",
    "                .select(df_contracts[\"*\"], address_df['address_id'])\n",
    "df_contracts = df_contracts.drop('from_address')\n",
    "\n",
    "tx_df = spark.read.format('jdbc').options(url=url, dbtable='Transactions', user=user, password=password).load()\n",
    "df_contracts = df_contracts.join(\n",
    "                tx_df,\n",
    "                (df_contracts.tx_hash == tx_df.tx_hash) & \n",
    "                (df_contracts.address_id == tx_df.from_id) &\n",
    "                (tx_df.to_id.isNull()), \n",
    "                'left').select(tx_df['tx_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cnx, cursor = db_conn.connect_db(db_config)\n",
    "db_conn.create_contracts_table(cursor)\n",
    "cursor.close()\n",
    "cnx.close()\n",
    "db_conn.export_df_to_sql(df_contracts, 'Contracts', db_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8666\n"
     ]
    }
   ],
   "source": [
    "scam_df = spark.read.csv('data/hoptrail_scam_dataset.csv', header=True)\n",
    "all_scam_addresses = pd.read_pickle('data/pickle_files/scam_users_and_contract_creators_alt.pkl')\n",
    "print(len(all_scam_addresses))\n",
    "row_schema = StructType([\n",
    "    StructField(\"address\", StringType(), True),\n",
    "])\n",
    "\n",
    "all_scam_addresses_rows = [Row(address=addr) for addr in all_scam_addresses]\n",
    "all_scam_addresses_df = spark.createDataFrame(all_scam_addresses_rows, schema=row_schema)\n",
    "scam_df = all_scam_addresses_df.join(scam_df, on='address', how='left')\n",
    "\n",
    "addresses_df = spark.read.format('jdbc').options(url=url, dbtable='Addresses', user=user, password=password).load()\n",
    "scam_df = scam_df.join(addresses_df, on='address', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "addresses_in_db = addresses_df.select(\"address\").rdd.flatMap(lambda x: x).collect()\n",
    "missing_addresses = [addr for addr in all_scam_addresses if addr not in addresses_in_db]\n",
    "with open('data/pickle_files/missing_addresses_alt.pickle', 'wb') as f:\n",
    "    pickle.dump(missing_addresses, f)\n",
    "\n",
    "scam_df = scam_df.dropna(subset=[\"address_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cnx, cursor = db_conn.connect_db(db_config)\n",
    "db_conn.create_scam_table(cursor)\n",
    "cursor.close()\n",
    "cnx.close()\n",
    "db_conn.export_df_to_sql(scam_df, 'ScamAddresses', db_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
