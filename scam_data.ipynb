{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import db_connection as db_conn\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from web3 import Web3\n",
    "# choose a Provider of your choice\n",
    "\n",
    "from alchemy_api import AlchemyApi\n",
    "import concurrent.futures\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "alchemy = AlchemyApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8473\n"
     ]
    }
   ],
   "source": [
    "scam_df = pd.read_csv('data/hoptrail_scam_dataset.csv')\n",
    "print(scam_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the scam addresses as list\n",
    "scam_addresses = scam_df['address'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_key_rotation(api_keys):\n",
    "    index = random.randint(0, len(api_keys) - 1)\n",
    "    api_key = api_keys[index]\n",
    "    return api_key\n",
    "\n",
    "user_addresses = []\n",
    "contract_addresses = []\n",
    "\n",
    "for address in scam_addresses:\n",
    "    check_sum_address = Web3.to_checksum_address(address)\n",
    "    api_key = api_key_rotation(alchemy.get_api_keys())\n",
    "    alchemy_url = alchemy.get_api_url() + api_key\n",
    "    w3 = Web3(Web3.HTTPProvider(alchemy_url))\n",
    "    response = w3.eth.get_code(check_sum_address)\n",
    "    if response.hex() == '0x':\n",
    "        print('User address: ', address)\n",
    "        user_addresses.append(address)\n",
    "    else:\n",
    "        print('Contract address: ', address)\n",
    "        contract_addresses.append(address)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_address_type(address, api_keys, alchemy_url):\n",
    "    check_sum_address = Web3.to_checksum_address(address)\n",
    "    api_key = api_key_rotation(api_keys)\n",
    "    full_url = alchemy_url + api_key\n",
    "    w3 = Web3(Web3.HTTPProvider(full_url))\n",
    "    response = w3.eth.get_code(check_sum_address)\n",
    "    return response.hex() == '0x'\n",
    "\n",
    "def api_key_rotation(api_keys):\n",
    "    index = random.randint(0, len(api_keys) - 1)\n",
    "    return api_keys[index]\n",
    "\n",
    "user_addresses = []\n",
    "contract_addresses = []\n",
    "api_keys = alchemy.get_api_keys()\n",
    "alchemy_url = alchemy.get_api_url()\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:\n",
    "    future_to_address = {executor.submit(get_address_type, address, api_keys, alchemy_url): address for address in scam_addresses}\n",
    "    for future in concurrent.futures.as_completed(future_to_address):\n",
    "        address = future_to_address[future]\n",
    "        is_user_address = future.result()\n",
    "        if is_user_address:\n",
    "            user_addresses.append(address)\n",
    "        else:\n",
    "            contract_addresses.append(address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User addresses:  7839\n",
      "Contract addresses:  634\n"
     ]
    }
   ],
   "source": [
    "print('User addresses: ', len(user_addresses))\n",
    "print('Contract addresses: ', len(contract_addresses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to pickle\n",
    "import pickle\n",
    "\n",
    "with open('data/pickle_files/user_addresses.pkl', 'wb') as f:\n",
    "    pickle.dump(user_addresses, f)\n",
    "\n",
    "with open('data/pickle_files/contract_addresses.pkl', 'wb') as f:\n",
    "    pickle.dump(contract_addresses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_addresses = pd.read_pickle('data/pickle_files/contract_addresses.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_list(input_list, chunk_size):\n",
    "    \"\"\"Yield successive chunks from input_list.\"\"\"\n",
    "    for i in range(0, len(input_list), chunk_size):\n",
    "        yield input_list[i:i + chunk_size]\n",
    "\n",
    "contract_addresses_chunks = list(chunk_list(contract_addresses, 5))\n",
    "\n",
    "etherscan_url = 'https://api.etherscan.io/api'\n",
    "ethscan_keys = ['VZWEQKFCC2K9TRJZKNQGFRD1K4TASQ7IMV','GVWFQX6V8TVYWR9PATBVNRHUUK4V4IKDBY','8R1AXV1QBC3P98P2EEJ2NMIG84RQS5BR8V']\n",
    "contract_creators = []\n",
    "\n",
    "for address_chunk in contract_addresses_chunks:\n",
    "    etherscan_params = {\n",
    "        'module': 'contract',\n",
    "        'action': 'getcontractcreation',\n",
    "        'contractaddresses': \",\".join(address_chunk),\n",
    "    }\n",
    "    headers = {'content-type': 'application/json', 'content-encoding': 'gzip', 'charset': 'utf-8'}\n",
    "\n",
    "    # Manually formatting the URL\n",
    "    etherscan_url += \"?module={}&action={}&contractaddresses={}&apikey={}\".format(\n",
    "        etherscan_params['module'],\n",
    "        etherscan_params['action'],\n",
    "        etherscan_params['contractaddresses'],\n",
    "        random.choice(ethscan_keys)\n",
    "    )\n",
    "\n",
    "    response = requests.get(etherscan_url, headers=headers)\n",
    "    etherscan_url = 'https://api.etherscan.io/api'\n",
    "    response_json = response.json()\n",
    "\n",
    "    for result in response_json['result']:\n",
    "        creator_details = {'contract_creator': result['contractCreator'], 'contract_address': result['contractAddress']}\n",
    "        contract_creators.append(creator_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_creators_df = pd.DataFrame(contract_creators)\n",
    "contract_creators_df = contract_creators_df.groupby('contract_creator')['contract_address'].agg(list).reset_index()\n",
    "contract_creators_df.to_parquet('data/parquet_files/contract_creators.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8385"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scam_users = pd.read_pickle('data/pickle_files/user_addresses.pkl')\n",
    "contract_creators_df = pd.read_parquet('data/parquet_files/contract_creators.parquet')\n",
    "scam_contract_creators = contract_creators_df['contract_creator'].tolist()\n",
    "\n",
    "scam_users += scam_contract_creators\n",
    "len(scam_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/04 22:39:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "db_con_test = db_conn.config_sql\n",
    "db_con_train = db_conn.config_train_sql\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"process_tx\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memory\", \"32g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"jdbc:mariadb://{db_con_test['host']}:{db_con_test['port']}/{db_con_test['database']}\"\n",
    "user = db_con_test['user']\n",
    "password = db_con_test['password']\n",
    "address_test_df = spark.read.format('jdbc').options(url=url, dbtable='Addresses', user=user, password=password).load()\n",
    "\n",
    "url = f\"jdbc:mariadb://{db_con_train['host']}:{db_con_train['port']}/{db_con_train['database']}\"\n",
    "user = db_con_train['user']\n",
    "password = db_con_train['password']\n",
    "address_train_df = spark.read.format('jdbc').options(url=url, dbtable='Addresses', user=user, password=password).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matches = address_train_df.filter(col('address').isin(scam_users))\n",
    "test_matches = address_test_df.filter(col('address').isin(scam_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_addresses = [row['address'] for row in train_matches.collect()]\n",
    "test_addresses = [row['address'] for row in test_matches.collect()]\n",
    "scam_addresses_in_train_test = list(set(train_addresses) | set(test_addresses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/pickle_files/scam_addresses_in_train_test.pkl', 'wb') as f:\n",
    "    pickle.dump(scam_addresses_in_train_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_scam_df = pd.read_csv('data/transaction_dataset.csv')\n",
    "original_scam_df = original_scam_df[original_scam_df['FLAG'] == 1]\n",
    "original_scam_addresses = original_scam_df['Address'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original scams:  2179\n",
      "Original scams in scam_users:  1928\n",
      "Filtered missing scams from scam_users:  251\n"
     ]
    }
   ],
   "source": [
    "filtered_original_scams = [scam for scam in original_scam_addresses if scam in scam_users]\n",
    "filtered_missing_scams = [scam for scam in original_scam_addresses if scam not in scam_users]\n",
    "\n",
    "print('Original scams: ', len(original_scam_addresses))\n",
    "print('Original scams in scam_users: ', len(filtered_original_scams))\n",
    "print('Filtered missing scams from scam_users: ', len(filtered_missing_scams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8636"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scam_users += filtered_missing_scams\n",
    "len(scam_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_ethereum_address(address):\n",
    "    match = re.match('^0x[a-fA-F0-9]{40}$', address)\n",
    "    return match is not None\n",
    "\n",
    "def is_valid_ethereum_tx_hash(tx_hash):\n",
    "    match = re.match('^0x[a-fA-F0-9]{64}$', tx_hash)\n",
    "    return match is not None\n",
    "\n",
    "invalid_addresses = []\n",
    "for address in scam_users:\n",
    "    if not is_valid_ethereum_address(address):\n",
    "        invalid_addresses.append(address)\n",
    "    if is_valid_ethereum_tx_hash(address):\n",
    "        invalid_addresses.append(address)\n",
    "\n",
    "invalid_addresses = list(set(invalid_addresses))\n",
    "\n",
    "# Remove invalid addresses from all_addresses list\n",
    "scam_users = [address for address in scam_users if address not in invalid_addresses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/pickle_files/scam_users_and_contract_creators.pkl', 'wb') as f:\n",
    "    pickle.dump(scam_users, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
